{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### locate_thresholds(df)\n",
    "locate the threshold that yields the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_thresholds(df):\n",
    "  thresholds = []\n",
    "\n",
    "  for i in range(len(df.eps.unique())):\n",
    "    epsilon = df.eps.unique()[i]\n",
    "\n",
    "    df_eps = df[df.eps == epsilon]\n",
    "    acc_one_eps = []\n",
    "    range_1 = np.linspace(0.1, 1, 10)\n",
    "    for threshold in range_1:\n",
    "      true_all = df_eps.true.to_numpy()\n",
    "      pred_all = df_eps.pred.apply(lambda x: 1 if x >= threshold else 0).to_numpy()\n",
    "      acc_one_eps.append(skm.accuracy_score(true_all, pred_all))\n",
    "    max_val = max(acc_one_eps)\n",
    "    max_index = acc_one_eps.index(max_val)\n",
    "\n",
    "    range_2 = np.linspace(range_1[max_index], range_1[max_index]+0.1, 11)\n",
    "    acc_one_eps = []\n",
    "    for threshold in range_2:\n",
    "      true_all = df_eps.true.to_numpy()\n",
    "      pred_all = df_eps.pred.apply(lambda x: 1 if x >= threshold else 0).to_numpy()\n",
    "      acc_one_eps.append(skm.accuracy_score(true_all, pred_all))\n",
    "    max_val = max(acc_one_eps)\n",
    "    max_index = acc_one_eps.index(max_val)\n",
    "\n",
    "    range_3 = np.linspace(range_2[max_index], range_2[max_index]+0.01, 11)\n",
    "    acc_one_eps = []\n",
    "    for threshold in range_3:\n",
    "      true_all = df_eps.true.to_numpy()\n",
    "      pred_all = df_eps.pred.apply(lambda x: 1 if x >= threshold else 0).to_numpy()\n",
    "      acc_one_eps.append(skm.accuracy_score(true_all, pred_all))\n",
    "    max_val = max(acc_one_eps)\n",
    "    max_index = acc_one_eps.index(max_val)\n",
    "\n",
    "    thresholds.append(range_3[max_index])\n",
    "    print('eps: {:.3f}, best threshold: {:.3f}, accuracy: {:.3f}'.format(epsilon, range_3[max_index], max_val))\n",
    "    \n",
    "  eps_arr = df.eps.unique()\n",
    "  th_arr = np.array(thresholds)\n",
    "  eps_th_dict = dict(zip(eps_arr, th_arr))    \n",
    "  return eps_th_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expand_dataframe(df, eps_th_dict)\n",
    "add five new columns:\n",
    "1. `pred_binary`: binary prediction (0 or 1); transformed from raw predictions (probabilities) based on the optimal threshold\n",
    "2. `tn`: true negative\n",
    "3. `fp`: false positive\n",
    "4. `fn`: false negative\n",
    "5. `tp`: true positive\n",
    "\n",
    "rename two columns for consistency:\n",
    "1. `race_white` to `race`\n",
    "2. `ethni` to `eth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dataframe(df, eps_th_dict):\n",
    "  df['pred_binary'] = df.apply(lambda row: 1 if row.pred >= eps_th_dict[row.eps] else 0, axis=1)\n",
    "\n",
    "  df['tn'] = df.apply(lambda row: 1 if (row.true == 0 and row.pred_binary == 0) else 0, axis=1)\n",
    "  df['fp'] = df.apply(lambda row: 1 if (row.true == 0 and row.pred_binary == 1) else 0, axis=1)\n",
    "  df['fn'] = df.apply(lambda row: 1 if (row.true == 1 and row.pred_binary == 0) else 0, axis=1)\n",
    "  df['tp'] = df.apply(lambda row: 1 if (row.true == 1 and row.pred_binary == 1) else 0, axis=1)\n",
    "\n",
    "  df.rename(columns={'race_white': 'race'}, inplace=True)\n",
    "  df.rename(columns={\"ethni\": \"eth\"}, inplace=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_results_csv(df)\n",
    "store cumulative results in a small csv file\n",
    "\n",
    "The small csv file contains one row for each epsilon value. Each row contains the mean and variance of the AUC value and three Disparate Impact values (Approval DI, False Negative DI, False Positive DI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_csv(df):\n",
    "  # eps and runs\n",
    "  eps_arr = df.eps.unique()\n",
    "  runs_arr = df.run.unique()\n",
    "\n",
    "  # auc column\n",
    "  auc_over_eps = []\n",
    "  for ith_eps in eps_arr:\n",
    "    df_ith_eps = df[df.eps == ith_eps]\n",
    "    auc_over_runs = []\n",
    "\n",
    "    for jth_run in runs_arr:\n",
    "      df_ith_eps_jth_run = df_ith_eps[df_ith_eps.run == jth_run]\n",
    "\n",
    "      true = df_ith_eps_jth_run.true.to_numpy()\n",
    "      prob = df_ith_eps_jth_run.pred.to_numpy()\n",
    "\n",
    "      auc_over_runs.append(skm.roc_auc_score(true, prob))\n",
    "\n",
    "    auc_over_eps.append(np.nanmean(auc_over_runs))\n",
    "\n",
    "  # di columns\n",
    "  metrics =['approval', 'fn', 'fp']\n",
    "  df_names = ['pred_binary', 'fn', 'fp']\n",
    "\n",
    "  mean_columns = []\n",
    "  mean_column_names = []\n",
    "\n",
    "  var_columns = []\n",
    "  var_column_names = []\n",
    "\n",
    "  for metric_index in range(len(metrics)):\n",
    "    metric = df_names[metric_index]\n",
    "\n",
    "    for feature in ['race', 'eth', 'sex']:\n",
    "      mean_di_metric_over_eps = []\n",
    "      var_di_metric_over_eps = []\n",
    "\n",
    "\n",
    "      for ith_eps in eps_arr:\n",
    "        df_ith_eps = df[df.eps == ith_eps]\n",
    "        di_metrics_over_runs = []\n",
    "\n",
    "        for jth_run in runs_arr:\n",
    "          di_ith_eps_jth_run = df_ith_eps[df_ith_eps.run == jth_run]\n",
    "\n",
    "          num_metric_proc = di_ith_eps_jth_run[(di_ith_eps_jth_run[col]==1) & (di_ith_eps_jth_run[feature]==0)].shape[0]\n",
    "          num_total_proc = di_ith_eps_jth_run[di_ith_eps_jth_run[col]==0].shape[0]\n",
    "\n",
    "          num_metric_unproc = di_ith_eps_jth_run[(di_ith_eps_jth_run[col]==1) & (di_ith_eps_jth_run[feature]==1)].shape[0]\n",
    "          num_total_unproc = di_ith_eps_jth_run[di_ith_eps_jth_run[col]==1].shape[0]\n",
    "          \n",
    "          # Suppress divide by 0 error\n",
    "          old_err_state = np.seterr(divide='raise')\n",
    "          ignored_states = np.seterr(**old_err_state)\n",
    "          \n",
    "          numerator = np.divide(num_metric_proc, num_total_proc)\n",
    "          denominator = np.divide(num_metric_unproc,num_total_unproc)\n",
    "\n",
    "          di_metrics_over_runs.append(np.divide(numerator,denominator))\n",
    "\n",
    "        mean_di_metric_over_eps.append(np.nanmean(di_metrics_over_runs))\n",
    "        var_di_metric_over_eps.append(np.nanvar(di_metrics_over_runs))\n",
    "\n",
    "      mean_columns.append(mean_di_metric_over_eps)\n",
    "      mean_column_name = 'mean_di_{}_{}'.format(metrics[metric_index], feature)\n",
    "      mean_column_names.append(mean_column_name)\n",
    "\n",
    "      var_columns.append(var_di_metric_over_eps)\n",
    "      var_column_name = 'var_di_{}_{}'.format(metrics[metric_index], feature)\n",
    "      var_column_names.append(var_column_name)\n",
    "\n",
    "  mean_df_cols_dict = dict(zip(np.array(mean_column_names), np.array(mean_columns)))\n",
    "  var_df_cols_dict = dict(zip(np.array(var_column_names), np.array(var_columns)))\n",
    "\n",
    "\n",
    "  # combine columns into a full dictionary\n",
    "  mean_df_cols_dict.update(var_df_cols_dict)        # variance columns\n",
    "  mean_df_cols_dict.update({'auc':auc_over_eps})    # auc column\n",
    "  mean_df_cols_dict.update({'eps':eps_arr})         # eps column\n",
    "\n",
    "  # transform dictionary into dataframe\n",
    "  results_df = pd.DataFrame(mean_df_cols_dict)\n",
    "\n",
    "  # save results to csv\n",
    "  outfile = 'results_{}.csv'.format(filename)\n",
    "  results_df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'nn_4states_balanced_action_cv_output.csv'\n",
    "df = pd.read_csv(filename)\n",
    "  \n",
    "eps_threshold_dict = locate_thresholds(df)\n",
    "expanded_df = expand_dataframe(df, eps_threshold_dict)\n",
    "create_results_csv(expanded_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
