{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## private_hmda_NC_cnn_v10\n",
    "### Neural Network using North Carolina HMDA LAR Data with Privacy\n",
    "\n",
    "#### Changes:\n",
    "* calculate DI with dataset v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import General Libraries \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Privacy Package\n",
    "from privacy.analysis.rdp_accountant import compute_rdp\n",
    "from privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dpsgd = True\n",
    "learning_rate = 0.15\n",
    "noise_multiplier = 0.5\n",
    "l2_norm_clip = 1.0\n",
    "batch_size = 1\n",
    "epochs = 2\n",
    "microbatches = 1\n",
    "model_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epsilon(steps):\n",
    "  \"\"\"Computes epsilon value for given hyperparameters.\"\"\"\n",
    "  if noise_multiplier == 0.0:\n",
    "    return float('inf')\n",
    "  orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "  sampling_probability = batch_size / 60000\n",
    "  rdp = compute_rdp(q=sampling_probability,\n",
    "                    noise_multiplier=noise_multiplier,\n",
    "                    steps=steps,\n",
    "                    orders=orders)\n",
    "  # Delta is set to 1e-5 because MNIST has 60000 training points.\n",
    "  return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(version):\n",
    "    data = pd.read_csv('hmda_nc_cleaned_v2.csv')\n",
    "\n",
    "    target = data['action_taken_name']\n",
    "    target = to_categorical(target) \n",
    "    predictors = data.drop(['action_taken_name'], axis=1)\n",
    "    predictors = predictors.drop(predictors.columns[0], axis=1)\n",
    "    \n",
    "    return predictors, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = DPGradientDescentGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=microbatches,\n",
    "    learning_rate=learning_rate,\n",
    "    unroll_microbatches=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178272 samples, validate on 44568 samples\n",
      "Epoch 1/2\n",
      "178272/178272 [==============================] - 409s 2ms/sample - loss: 6.0868 - acc: 0.6223 - val_loss: 3.1742 - val_acc: 0.8031\n",
      "Epoch 2/2\n",
      "178272/178272 [==============================] - 414s 2ms/sample - loss: 11.4395 - acc: 0.2903 - val_loss: 12.7016 - val_acc: 0.2120\n",
      "acc: 20.91%\n",
      "For delta=1e-5, the current epsilon is: 2.77\n"
     ]
    }
   ],
   "source": [
    "X, Y = data_processing(i)\n",
    "n_cols = X.shape[1]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape = (n_cols,)),\n",
    "      tf.keras.layers.Dense(25, activation='relu'),\n",
    "      tf.keras.layers.Dense(20, activation='relu'),\n",
    "      tf.keras.layers.Dense(2, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state = 0)\n",
    "model.fit(x_train, y_train, validation_split=0.2, epochs=epochs, batch_size=batch_size, verbose = 1)\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "eps = compute_epsilon(epochs * 60000 // batch_size)\n",
    "print('For delta=1e-5, the current epsilon is: %.2f' % eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "\n",
    "y_true_one_col = []\n",
    "\n",
    "for row in y_true:\n",
    "    if row[1] == 1:\n",
    "        y_true_one_col.append(1)\n",
    "    else:\n",
    "        y_true_one_col.append(0)\n",
    "        \n",
    "y_true_one_col = np.asarray(y_true_one_col)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Overall confusion matrix:\n",
      "[[ 9658   297]\n",
      " [43767  1989]]\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-' * 30)\n",
    "print('Overall confusion matrix:')\n",
    "print(confusion_matrix(y_true_one_col, y_pred))\n",
    "print('-' * 30)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mat(var):\n",
    "    predictors = x_test[x_test[var] == 1]\n",
    "    target = y_test[x_test[var] == 1]\n",
    "    \n",
    "    y_true = target\n",
    "\n",
    "    y_true_one_col = []\n",
    "\n",
    "    for row in y_true:\n",
    "        if row[1] == 1:\n",
    "            y_true_one_col.append(1)\n",
    "        else:\n",
    "            y_true_one_col.append(0)\n",
    "\n",
    "    y_true_one_col = np.asarray(y_true_one_col)\n",
    "\n",
    "    y_pred = model.predict(predictors)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    return confusion_matrix(y_true_one_col, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Confusion matrix by features:\n",
      "\n",
      "applicant_race_name_1_0\n",
      "[[2845  170]\n",
      " [6963  698]]\n",
      "\n",
      "applicant_race_name_1_1\n",
      "[[ 6813   127]\n",
      " [36804  1291]]\n",
      "\n",
      "applicant_ethnicity_name_Hispanic or Latino\n",
      "[[ 515    6]\n",
      " [2174   36]]\n",
      "\n",
      "applicant_ethnicity_name_Not Hispanic or Latino\n",
      "[[ 9143   291]\n",
      " [41593  1953]]\n",
      "\n",
      "applicant_sex_name_Female\n",
      "[[ 3614   132]\n",
      " [14147   672]]\n",
      "\n",
      "applicant_sex_name_Male\n",
      "[[ 6044   165]\n",
      " [29620  1317]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "features = ['applicant_race_name_1_0', 'applicant_race_name_1_1', 'applicant_ethnicity_name_Hispanic or Latino', 'applicant_ethnicity_name_Not Hispanic or Latino', 'applicant_sex_name_Female', 'applicant_sex_name_Male']\n",
    "print('-' * 30)\n",
    "print('Confusion matrix by features:')\n",
    "for ft in features:\n",
    "    print()\n",
    "    print(ft)\n",
    "    print(conf_mat(ft))\n",
    "print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm stands for confusion matrix\n",
    "\n",
    "def calc_di2(prot_cm, unprot_cm):\n",
    "    prot_total = np.sum(prot_cm)\n",
    "    unprot_total = np.sum(unprot_cm)\n",
    "    \n",
    "    prot_1 = np.sum(prot_cm, axis = 0)[1]\n",
    "    unprot_1 = np.sum(unprot_cm, axis = 0)[1]\n",
    "    \n",
    "    return (prot_1/prot_total)/(unprot_1/unprot_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_di(prot_var_name, unprot_var_name):\n",
    "    prot_df = x_test[prot_var_name]\n",
    "    unprot_df = x_test[unprot_var_name]\n",
    "    \n",
    "    prot_total = prot_df.value_counts()[1]\n",
    "    unprot_total = unprot_df.value_counts()[1]\n",
    "    \n",
    "    prot_pred = np.argmax(model.predict(x_test[prot_df == 1]), axis = 1)\n",
    "    unprot_pred = np.argmax(model.predict(x_test[unprot_df == 1]), axis = 1)\n",
    "    \n",
    "    prot_1 = np.count_nonzero(prot_pred)\n",
    "    unprot_1 = np.count_nonzero(unprot_pred)\n",
    "    \n",
    "    return (prot_1/prot_total)/(unprot_1/unprot_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "DI by features:\n",
      "\n",
      "DI between 'applicant_race_name_1_0' and 'applicant_race_name_1_1'\n",
      "2.5821715765982622\n",
      "\n",
      "DI between 'applicant_ethnicity_name_Hispanic or Latino' and 'applicant_ethnicity_name_Not Hispanic or Latino'\n",
      "0.36309200954773574\n",
      "\n",
      "DI between 'applicant_sex_name_Female' and 'applicant_sex_name_Male'\n",
      "1.0854877980964137\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-' * 30)\n",
    "print('DI by features:')\n",
    "for i in range(0, len(features), 2):\n",
    "    print()\n",
    "    print('DI between \\'{:s}\\' and \\'{:s}\\''.format(features[i], features[i+1]))\n",
    "    print(calc_di(features[i], features[i+1]))\n",
    "print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
