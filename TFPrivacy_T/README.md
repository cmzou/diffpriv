# Outline
`BankingNN.ipynb`: A notebook of my experiments with [TensorFlow Privacy](https://github.com/tensorflow/privacy) and [VisualizeNN](https://github.com/jzliu-100/visualize-neural-network)

`Interpretation.ipynb`: A notebook of my experiments with [LIME](https://github.com/marcotcr/lime)

`InterpretationScript.ipynb`: The streamlined version of `Interpretation.ipynb` setup to run on a compute cluster

## BankingNN
My goal of this notebook was to implement a testing framework that would allow people to implement a couple different optimizers and loss functions. There were some inherent limitations to this system however. In the current version, the model architecture is hardcoded and must remain the same for all runs. Additionally, this testing framework only measures accuracy. While this can be an effective testing method for many general applications, when checking fairness, testing only accuracy can have leave out the whole story. 

In it's current state, the framework is unable to output false positives and negatives, or true positives and negatives. Additionally, the framework is limited to only outputting metrics for the group as a whole rather than defined subgroups that researches may be interested such as race or sex. As a result, this framework was scrapped for the alternative found in [`../py_tf_yyz`](https://github.com/cmzou/diffpriv/tree/master/py_tf_yyz).

## Interpretation
This notebook's purpose was to utilize LIME's framework of "explaining" model predictions and expand it to show how the features with the most impact might change between models. The purpose of this was the explore how a private model trained on the exact same data differed from a non-private model. We arbitrarily chose to look at the top five features (as ranked by LIME) as thats what made the graphs that allowed us to see the most variation without getting too busy. This script shows what each model's top features are and colorcodes them based on their relation to the applicant: Green - Applicant Controlled Info (Income, Loan Amount, ...), Red - Protected Classes (Race, Sex, Ethnicity), Black - Other (Median Income, # of Families in Tract, ...). We look at this over five models for the same optimizer so that we can see the variation in the valued features within a single epsilon in the private models. We believe this is due to the random noise added to the gradient by TensorFlow Privacy which could cause the weights to end at a point such that previously arbitrary features are valued more. 

One of the possible confounding factors for the extreme variation in top features some of the private models exhibited is the drop in accuracy caused by introducing noise into the system. Many of our runs on a balanced by action dataset resulted in a drop in accuracy to 50%, no better than randomly guessing the label.

## InterpretationScript
This is a simplified version of `Interpretation.ipynb` which was created to run multiple (in our case, three) optimizers five different times to look at the variation between the optimizers and models themselves. In this file, we take the top features generated by the frist non-private model and apply them to all the other models. This gives us a consistent way to look at how all of the models differ. One of the limitations of this system however is that you are limited in view to whatever the first non-private model chooses as it's top features, although one should be able to hardcode in a list of features that they would like to look at for all models. 
